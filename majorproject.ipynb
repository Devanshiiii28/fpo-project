{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdd3ec5-1301-4e48-8f8e-42c6f606d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Farmer_ID</th>\n",
       "      <th>Village_ID</th>\n",
       "      <th>District</th>\n",
       "      <th>State</th>\n",
       "      <th>Group</th>\n",
       "      <th>FPO_Member</th>\n",
       "      <th>WDC_Intervention_Area</th>\n",
       "      <th>Age_of_Farmer</th>\n",
       "      <th>Land_Size_Hectares</th>\n",
       "      <th>Farm_Size_Acres</th>\n",
       "      <th>...</th>\n",
       "      <th>Time_Period</th>\n",
       "      <th>Irrigation_Source_Before</th>\n",
       "      <th>Irrigation_Source_After</th>\n",
       "      <th>Training_Received</th>\n",
       "      <th>Crop_Diversification</th>\n",
       "      <th>Fertilizer_Used_Kg_per_Hectare</th>\n",
       "      <th>Pesticide_Used_Liters_per_Hectare</th>\n",
       "      <th>Yield_Kg_Per_Hectare</th>\n",
       "      <th>Cost_of_Cultivation</th>\n",
       "      <th>Income_Annual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FARM0001</td>\n",
       "      <td>MUR03</td>\n",
       "      <td>Muradnagar</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>42</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.66</td>\n",
       "      <td>...</td>\n",
       "      <td>After</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>128.01</td>\n",
       "      <td>4.48</td>\n",
       "      <td>9613.59</td>\n",
       "      <td>23305.12</td>\n",
       "      <td>133212.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FARM0001</td>\n",
       "      <td>MUR03</td>\n",
       "      <td>Muradnagar</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>42</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.66</td>\n",
       "      <td>...</td>\n",
       "      <td>Before</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>122.99</td>\n",
       "      <td>4.70</td>\n",
       "      <td>8251.22</td>\n",
       "      <td>23828.24</td>\n",
       "      <td>117773.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FARM0002</td>\n",
       "      <td>PUN04</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>40</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.89</td>\n",
       "      <td>...</td>\n",
       "      <td>After</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>123.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>4694.72</td>\n",
       "      <td>11857.18</td>\n",
       "      <td>64491.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FARM0002</td>\n",
       "      <td>PUN04</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>40</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.89</td>\n",
       "      <td>...</td>\n",
       "      <td>Before</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>Rainfed</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>118.70</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4148.11</td>\n",
       "      <td>12725.25</td>\n",
       "      <td>56762.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FARM0003</td>\n",
       "      <td>MUR02</td>\n",
       "      <td>Muradnagar</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>56</td>\n",
       "      <td>3.68</td>\n",
       "      <td>9.09</td>\n",
       "      <td>...</td>\n",
       "      <td>After</td>\n",
       "      <td>Canal</td>\n",
       "      <td>Canal</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>107.31</td>\n",
       "      <td>4.30</td>\n",
       "      <td>18219.71</td>\n",
       "      <td>40492.74</td>\n",
       "      <td>257477.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Farmer_ID Village_ID    District          State      Group FPO_Member  \\\n",
       "0  FARM0001      MUR03  Muradnagar  Uttar Pradesh  Treatment        YES   \n",
       "1  FARM0001      MUR03  Muradnagar  Uttar Pradesh  Treatment        YES   \n",
       "2  FARM0002      PUN04        Pune    Maharashtra  Treatment        YES   \n",
       "3  FARM0002      PUN04        Pune    Maharashtra  Treatment        YES   \n",
       "4  FARM0003      MUR02  Muradnagar  Uttar Pradesh  Treatment        YES   \n",
       "\n",
       "  WDC_Intervention_Area  Age_of_Farmer  Land_Size_Hectares  Farm_Size_Acres  \\\n",
       "0                   YES             42                2.29             5.66   \n",
       "1                   YES             42                2.29             5.66   \n",
       "2                   YES             40                1.17             2.89   \n",
       "3                   YES             40                1.17             2.89   \n",
       "4                   YES             56                3.68             9.09   \n",
       "\n",
       "   ... Time_Period Irrigation_Source_Before Irrigation_Source_After  \\\n",
       "0  ...       After                  Rainfed                 Rainfed   \n",
       "1  ...      Before                  Rainfed                 Rainfed   \n",
       "2  ...       After                  Rainfed                 Rainfed   \n",
       "3  ...      Before                  Rainfed                 Rainfed   \n",
       "4  ...       After                    Canal                   Canal   \n",
       "\n",
       "   Training_Received  Crop_Diversification  Fertilizer_Used_Kg_per_Hectare  \\\n",
       "0                YES                    NO                          128.01   \n",
       "1                 NO                    NO                          122.99   \n",
       "2                YES                    NO                          123.05   \n",
       "3                 NO                    NO                          118.70   \n",
       "4                YES                    NO                          107.31   \n",
       "\n",
       "   Pesticide_Used_Liters_per_Hectare  Yield_Kg_Per_Hectare  \\\n",
       "0                               4.48               9613.59   \n",
       "1                               4.70               8251.22   \n",
       "2                               5.21               4694.72   \n",
       "3                               5.49               4148.11   \n",
       "4                               4.30              18219.71   \n",
       "\n",
       "  Cost_of_Cultivation Income_Annual  \n",
       "0            23305.12     133212.18  \n",
       "1            23828.24     117773.85  \n",
       "2            11857.18      64491.81  \n",
       "3            12725.25      56762.10  \n",
       "4            40492.74     257477.69  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The file path from your error message\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "\n",
    "# Use pd.read_excel() because the file is likely an Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 5 rows to confirm it loaded correctly\n",
    "print(\"Dataset loaded successfully!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be761445-4a8c-4cc5-a04f-3333a429f718",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1912955877.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 48\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(results.summary())|\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming 'df' is your dataframe from the previous step where you loaded the data\n",
    "\n",
    "# --- Step 1: Prepare the Data for Modeling ---\n",
    "\n",
    "# For this model, we need to convert our text-based categories into numbers.\n",
    "# The get_dummies() function does this automatically. For example, it will convert the 'Group'\n",
    "# column into two new columns: 'Group_Treatment' and 'Group_Control', with 1s or 0s.\n",
    "df_model = pd.get_dummies(df, columns=['Group', 'Time_Period'], drop_first=True)\n",
    "\n",
    "# Let's rename the new columns for clarity\n",
    "df_model.rename(columns={'Group_Treatment': 'Is_Treatment_Group', 'Time_Period_Before': 'Is_Before_Period'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- Step 2: Define the Model Variables ---\n",
    "\n",
    "# The 'dependent' variable (y) is what we want to predict or explain: Annual Income.\n",
    "y = df_model['Income_Annual']\n",
    "\n",
    "# The 'independent' variables (X) are the factors we use to explain the income.\n",
    "# We include our key variable 'Is_Treatment_Group' and other important control variables.\n",
    "X = df_model[[\n",
    "    'Is_Treatment_Group',\n",
    "    'Land_Size_Hectares',\n",
    "    'Rainfall_Annual_mm',\n",
    "    'Soil_Health_Score',\n",
    "    'Off_Farm_Income'\n",
    "]]\n",
    "\n",
    "# We also need to add a 'constant' to our model, which is like the baseline income.\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "\n",
    "# --- Step 3: Run the Regression Model ---\n",
    "\n",
    "# We create the Ordinary Least Squares (OLS) model\n",
    "model = sm.OLS(y, X)\n",
    "\n",
    "# We 'fit' the model to our data, which means it performs the calculations.\n",
    "results = model.fit()\n",
    "\n",
    "\n",
    "# --- Step 4: Print and Analyze the Results ---\n",
    "\n",
    "print(\"--- Regression Model Results ---\")\n",
    "print(results.summary())|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48de319-2562-4a1f-9d6e-afc1d900d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming 'df' is your dataframe from the previous step where you loaded the data\n",
    "\n",
    "# --- Step 1: Prepare the Data for Modeling ---\n",
    "df_model = pd.get_dummies(df, columns=['Group', 'Time_Period'], drop_first=True)\n",
    "df_model.rename(columns={'Group_Treatment': 'Is_Treatment_Group', 'Time_Period_Before': 'Is_Before_Period'}, inplace=True)\n",
    "\n",
    "# --- Step 2: Define the Model Variables ---\n",
    "y = df_model['Income_Annual']\n",
    "X = df_model[[\n",
    "    'Is_Treatment_Group',\n",
    "    'Land_Size_Hectares',\n",
    "    'Rainfall_Annual_mm',\n",
    "    'Soil_Health_Score',\n",
    "    'Off_Farm_Income'\n",
    "]]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# --- Step 3: Run the Regression Model ---\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# --- Step 4: Print and Analyze the Results (Corrected Line) ---\n",
    "print(\"--- Regression Model Results ---\")\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ba7c0-1ace-4eff-8234-f7b9e51cec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEBUGGING STEP ---\n",
    "# This code will help us find the column with the wrong data type.\n",
    "\n",
    "# Prepare the data for modeling (same as before)\n",
    "df_model = pd.get_dummies(df, columns=['Group', 'Time_Period'], drop_first=True)\n",
    "\n",
    "# Define the variables (same as before)\n",
    "y = df_model['Income_Annual']\n",
    "X = df_model[[\n",
    "    'Group_Treatment',    # Using the default name from get_dummies\n",
    "    'Land_Size_Hectares',\n",
    "    'Rainfall_Annual_mm',\n",
    "    'Soil_Health_Score',\n",
    "    'Off_Farm_Income'\n",
    "]]\n",
    "\n",
    "# Print the data types of our selected columns\n",
    "print(\"--- Data Type of y (our target) ---\")\n",
    "print(y.dtype)\n",
    "print(\"\\n--- Data Types of X (our features) ---\")\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4a403-6e5a-49cc-ade0-3882df7374e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming 'df' is your loaded dataframe\n",
    "\n",
    "# --- Step 1: Prepare the Data for Modeling (Robust Version) ---\n",
    "df_model = pd.get_dummies(df, columns=['Group', 'Time_Period'], drop_first=True)\n",
    "\n",
    "\n",
    "# --- Step 2: Define the Model Variables ---\n",
    "\n",
    "# Define y and ensure it's a numeric type\n",
    "y = df_model['Income_Annual'].astype(float)\n",
    "\n",
    "# Define X and ensure ALL columns are numeric\n",
    "# We use 'Group_Treatment', which is the default name created by get_dummies\n",
    "X = df_model[[\n",
    "    'Group_Treatment',\n",
    "    'Land_Size_Hectares',\n",
    "    'Rainfall_Annual_mm',\n",
    "    'Soil_Health_Score',\n",
    "    'Off_Farm_Income'\n",
    "]].astype(float) # .astype(float) makes sure all columns are numbers\n",
    "\n",
    "# Add the constant\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "\n",
    "# --- Step 3: Run the Regression Model ---\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "\n",
    "# --- Step 4: Print and Analyze the Results ---\n",
    "print(\"--- Regression Model Results ---\")\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df54ac2-9f67-4bed-a06e-2e17a34de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -- Data Loading --\n",
    "# Ensure the CSV file is in the same folder as your notebook\n",
    "df = pd.read_csv('fpo_impact_data_enhanced.csv')\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEEPER EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 1: Deeper EDA ---\")\n",
    "\n",
    "# 1a. Correlation Heatmap for Numerical Columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=np.number)\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 1b. Viewing Categorical Columns vs. Income\n",
    "# We will check how 'Primary_Crop' affects income\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x='Primary_Crop', y='Income_Annual', data=df)\n",
    "plt.title('Annual Income Distribution by Primary Crop', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Annual Income (INR)')\n",
    "plt.show()\n",
    "\n",
    "# 1c. Location-based Analysis (Charts instead of Maps)\n",
    "# We don't have lat/lon for maps, but we can analyze by district\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_income_by_district = df.groupby('District')['Income_Annual'].mean().sort_values(ascending=False)\n",
    "avg_income_by_district.plot(kind='bar')\n",
    "plt.title('Average Annual Income by District', fontsize=16)\n",
    "plt.ylabel('Average Annual Income (INR)')\n",
    "plt.xlabel('District')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: OUTLIER DETECTION AND REMOVAL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 2: Outlier Removal ---\")\n",
    "\n",
    "# We will focus on outliers in our target variable, 'Income_Annual'\n",
    "Q1 = df['Income_Annual'].quantile(0.25)\n",
    "Q3 = df['Income_Annual'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "original_rows = df.shape[0]\n",
    "df_cleaned = df[(df['Income_Annual'] >= lower_bound) & (df['Income_Annual'] <= upper_bound)]\n",
    "cleaned_rows = df_cleaned.shape[0]\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\")\n",
    "print(f\"Number of rows after removing outliers: {cleaned_rows}\")\n",
    "print(f\"Number of outliers removed: {original_rows - cleaned_rows}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: PREPARING DATA FOR MODEL COMPARISON\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 3: Preparing Data for Modeling ---\")\n",
    "\n",
    "# We use the cleaned dataframe\n",
    "# Convert all relevant categorical columns to numbers\n",
    "df_model = pd.get_dummies(df_cleaned.drop(columns=['Farmer_ID', 'Village_ID']), drop_first=True)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "y = df_model['Income_Annual']\n",
    "X = df_model.drop('Income_Annual', axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data is preprocessed and split into training and testing sets.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: ALGORITHM COMPARISON TO SELECT THE BEST MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 4: Comparing ML Algorithms ---\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the models we want to test\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through the models, train them, and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R-squared': r2}\n",
    "\n",
    "# Print the results in a clean table\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='RMSE', ascending=True)\n",
    "print(\"\\n--- Model Comparison Results ---\")\n",
    "print(results_df)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "best_model_rmse = results_df.iloc[0]['RMSE']\n",
    "print(f\"\\nBased on the results, the best algorithm is **{best_model_name}** with an RMSE of ₹{best_model_rmse:,.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c9eca-8a46-4180-998a-06fc067cfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the full path to your file and the correct function (read_excel)\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"Excel file loaded successfully from Desktop!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file was not found at that specific path. Please double-check the file name and location.\")\n",
    "\n",
    "# You can now display the first 5 rows to check\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250f3b6-0b78-4990-8de4-03e7a806ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682de37-69a4-4f3c-8fff-dadf1f0784dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -- Corrected Data Loading --\n",
    "# Use the full path to your file and the correct function (read_excel)\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEEPER EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 1: Deeper EDA ---\")\n",
    "\n",
    "# 1a. Correlation Heatmap for Numerical Columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_cols = df.select_dtypes(include=np.number)\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 1b. Viewing Categorical Columns vs. Income\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x='Primary_Crop', y='Income_Annual', data=df)\n",
    "plt.title('Annual Income Distribution by Primary Crop', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Annual Income (INR)')\n",
    "plt.show()\n",
    "\n",
    "# 1c. Location-based Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_income_by_district = df.groupby('District')['Income_Annual'].mean().sort_values(ascending=False)\n",
    "avg_income_by_district.plot(kind='bar')\n",
    "plt.title('Average Annual Income by District', fontsize=16)\n",
    "plt.ylabel('Average Annual Income (INR)')\n",
    "plt.xlabel('District')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: OUTLIER DETECTION AND REMOVAL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 2: Outlier Removal ---\")\n",
    "\n",
    "Q1 = df['Income_Annual'].quantile(0.25)\n",
    "Q3 = df['Income_Annual'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "original_rows = df.shape[0]\n",
    "df_cleaned = df[(df['Income_Annual'] >= lower_bound) & (df['Income_Annual'] <= upper_bound)]\n",
    "cleaned_rows = df_cleaned.shape[0]\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\")\n",
    "print(f\"Number of rows after removing outliers: {cleaned_rows}\")\n",
    "print(f\"Number of outliers removed: {original_rows - cleaned_rows}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: PREPARING DATA FOR MODEL COMPARISON\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 3: Preparing Data for Modeling ---\")\n",
    "\n",
    "df_model = pd.get_dummies(df_cleaned.drop(columns=['Farmer_ID', 'Village_ID']), drop_first=True)\n",
    "y = df_model['Income_Annual']\n",
    "X = df_model.drop('Income_Annual', axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data is preprocessed and split into training and testing sets.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: ALGORITHM COMPARISON TO SELECT THE BEST MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 4: Comparing ML Algorithms ---\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R-squared': r2}\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='RMSE', ascending=True)\n",
    "print(\"\\n--- Model Comparison Results ---\")\n",
    "print(results_df)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "best_model_rmse = results_df.iloc[0]['RMSE']\n",
    "print(f\"\\nBased on the results, the best algorithm is **{best_model_name}** with an RMSE of ₹{best_model_rmse:,.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36c868-1769-45ab-9037-dc0c03c15833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f99e4-035e-42ac-a8ab-f97474572436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DATA PREPARATION (Same as before)\n",
    "# ==============================================================================\n",
    "# -- Corrected Data Loading --\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# -- Outlier Removal --\n",
    "Q1 = df['Income_Annual'].quantile(0.25)\n",
    "Q3 = df['Income_Annual'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df_cleaned = df[(df['Income_Annual'] >= lower_bound) & (df['Income_Annual'] <= upper_bound)]\n",
    "\n",
    "# -- Preprocessing --\n",
    "df_model = pd.get_dummies(df_cleaned.drop(columns=['Farmer_ID', 'Village_ID']), drop_first=True)\n",
    "y = df_model['Income_Annual']\n",
    "X = df_model.drop('Income_Annual', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: HYPERPARAMETER TUNING WITH RANDOMIZED SEARCH\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 2: Hyperparameter Tuning for Gradient Boosting ---\")\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "# n_iter=20 means it will try 20 different combinations of parameters\n",
    "# cv=5 means it will use 5-fold cross-validation\n",
    "# n_jobs=-1 uses all available CPU cores to speed up the process\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the random search to the data (this will take a few minutes)\n",
    "print(\"Running randomized search to find the best model parameters...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EVALUATE THE TUNED MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting Part 3: Evaluating the Fine-Tuned Model ---\")\n",
    "\n",
    "# Get the best model found by the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the new, improved performance metrics\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(\"\\n--- Tuned Model Performance ---\")\n",
    "print(f\"Tuned Model R-squared (R²): {r2_tuned:.4f}\")\n",
    "print(f\"Tuned Model RMSE: ₹{rmse_tuned:,.2f}\")\n",
    "\n",
    "# You can compare this to the results from the previous, untuned model\n",
    "# to see the improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e8972-bd2e-41d8-ac91-a73cb50c0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 4: COMPARE TUNED MODEL WITH DEFAULT MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Comparing Model Performance ---\")\n",
    "\n",
    "# --- Step 1: Train the Default (Untuned) Model ---\n",
    "# Initialize a new model with default settings\n",
    "default_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Train it on the same training data\n",
    "default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the same test data\n",
    "y_pred_default = default_model.predict(X_test)\n",
    "\n",
    "# Calculate its performance metrics\n",
    "rmse_default = np.sqrt(mean_squared_error(y_test, y_pred_default))\n",
    "r2_default = r2_score(y_test, y_pred_default)\n",
    "\n",
    "\n",
    "# --- Step 2: Print a Comparison Table ---\n",
    "# The 'rmse_tuned' and 'r2_tuned' variables should already exist from your previous cell\n",
    "\n",
    "print(\"\\n--- Performance Comparison ---\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"| Metric      | Default Model     | Tuned Model       |\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"| R-squared   | {r2_default:<17.4f} | {r2_tuned:<17.4f} |\")\n",
    "print(f\"| RMSE (₹)    | {rmse_default:<17,.2f} | {rmse_tuned:<17,.2f} |\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "# --- Step 3: Calculate and Print the Improvement ---\n",
    "rmse_improvement = ((rmse_default - rmse_tuned) / rmse_default) * 100\n",
    "print(f\"\\nImprovement from Tuning:\")\n",
    "print(f\"The tuned model's RMSE is ₹{rmse_default - rmse_tuned:,.2f} lower than the default model.\")\n",
    "print(f\"This represents a {rmse_improvement:.2f}% reduction in prediction error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a269b9-d7c2-4d86-ad95-6ac500d3e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the full path to your file and the correct function (read_excel)\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Calculate the average annual income from your dataset\n",
    "average_annual_income = df['Income_Annual'].mean()\n",
    "\n",
    "# Convert it to a monthly average\n",
    "average_monthly_income = average_annual_income / 12\n",
    "\n",
    "print(f\"The average monthly income in your dataset is: ₹{average_monthly_income:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e5d2a-35ec-41d0-8ddc-34c1d602d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -- Corrected Data Loading --\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# --- 1. District-wise Income Comparison ---\n",
    "# This plot shows the \"Before\" and \"After\" income for the Treatment group in each district.\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(\n",
    "    x='District',\n",
    "    y='Income_Annual',\n",
    "    hue='Time_Period',\n",
    "    data=df[df['Group'] == 'Treatment'], # We only care about the change in the Treatment group\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('District-wise Income: Before vs. After Intervention (Treatment Group)', fontsize=16)\n",
    "plt.ylabel('Average Annual Income (INR)')\n",
    "plt.xlabel('District')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Time Period')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. \"Map-like\" Visualization of Project Impact ---\n",
    "# Since we don't have geographic coordinates for a real map, we can create\n",
    "# a chart that shows the *net increase* in income for each district.\n",
    "# This visually represents the \"impact hot-spots\".\n",
    "\n",
    "# First, calculate the average 'Before' and 'After' income per district\n",
    "pivot_df = df[df['Group'] == 'Treatment'].pivot_table(\n",
    "    values='Income_Annual',\n",
    "    index='District',\n",
    "    columns='Time_Period',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Calculate the net increase\n",
    "pivot_df['Income_Increase'] = pivot_df['After'] - pivot_df['Before']\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 7))\n",
    "pivot_df['Income_Increase'].sort_values(ascending=False).plot(\n",
    "    kind='bar',\n",
    "    color=sns.color_palette(\"plasma\", len(pivot_df))\n",
    ")\n",
    "plt.title('Net Annual Income Increase per District from WDC 2.0 Project', fontsize=16)\n",
    "plt.ylabel('Average Income Increase (INR)')\n",
    "plt.xlabel('District')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Comparison Graph by Primary Crop ---\n",
    "# This shows which crops benefited the most from the intervention.\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(\n",
    "    x='Primary_Crop',\n",
    "    y='Income_Annual',\n",
    "    hue='Time_Period',\n",
    "    data=df[df['Group'] == 'Treatment'],\n",
    "    palette='magma'\n",
    ")\n",
    "plt.title('Crop-wise Income: Before vs. After Intervention (Treatment Group)', fontsize=16)\n",
    "plt.ylabel('Average Annual Income (INR)')\n",
    "plt.xlabel('Primary Crop')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Time Period')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5865f32-f1c5-42b9-94c9-0b1d5bdb34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_impact_summary(df, group_by_param):\n",
    "    \"\"\"\n",
    "    Calculates the before and after income evaluation for the Treatment group,\n",
    "    broken down by a specified parameter (e.g., 'District' or 'Primary_Crop').\n",
    "    \"\"\"\n",
    "    # Filter for only the farmers who received the intervention\n",
    "    treatment_df = df[df['Group'] == 'Treatment'].copy()\n",
    "    \n",
    "    # Create a pivot table to get Before vs. After income\n",
    "    summary_pivot = treatment_df.pivot_table(\n",
    "        values='Income_Annual',\n",
    "        index=group_by_param,\n",
    "        columns='Time_Period',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Calculate the net and percentage increase in income\n",
    "    summary_pivot['Net_Income_Increase'] = summary_pivot['After'] - summary_pivot['Before']\n",
    "    summary_pivot['Pct_Income_Increase'] = (summary_pivot['Net_Income_Increase'] / summary_pivot['Before']) * 100\n",
    "    \n",
    "    # Format the table for better readability\n",
    "    summary_pivot = summary_pivot.sort_values(by='Net_Income_Increase', ascending=False)\n",
    "    summary_pivot.rename(columns={'Before': 'Avg Income Before (INR)', 'After': 'Avg Income After (INR)'}, inplace=True)\n",
    "    \n",
    "    return summary_pivot.style.format({\n",
    "        'Avg Income Before (INR)': '₹{:,.2f}',\n",
    "        'Avg Income After (INR)': '₹{:,.2f}',\n",
    "        'Net_Income_Increase': '₹{:,.2f}',\n",
    "        'Pct_Income_Increase': '{:.2f}%'\n",
    "    })\n",
    "\n",
    "# --- HOW TO USE ---\n",
    "# Load your data\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 1. Final Evaluation by District\n",
    "print(\"--- Final Income Evaluation by District ---\")\n",
    "district_summary = generate_impact_summary(df, 'District')\n",
    "display(district_summary)\n",
    "\n",
    "# 2. Final Evaluation by Primary Crop\n",
    "print(\"\\n--- Final Income Evaluation by Primary Crop ---\")\n",
    "crop_summary = generate_impact_summary(df, 'Primary_Crop')\n",
    "display(crop_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e5d7b-db84-4cba-8140-bb9d2c58b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_impact_summary(df, group_by_param, styled=True):\n",
    "    \"\"\"\n",
    "    Calculates the before and after income evaluation for the Treatment group.\n",
    "    Returns a styled DataFrame for display or a raw DataFrame for plotting.\n",
    "    \"\"\"\n",
    "    treatment_df = df[df['Group'] == 'Treatment'].copy()\n",
    "    \n",
    "    summary_pivot = treatment_df.pivot_table(\n",
    "        values='Income_Annual',\n",
    "        index=group_by_param,\n",
    "        columns='Time_Period',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    summary_pivot['Net_Income_Increase'] = summary_pivot['After'] - summary_pivot['Before']\n",
    "    summary_pivot['Pct_Income_Increase'] = (summary_pivot['Net_Income_Increase'] / summary_pivot['Before']) * 100\n",
    "    \n",
    "    summary_pivot = summary_pivot.sort_values(by='Net_Income_Increase', ascending=False)\n",
    "    summary_pivot.rename(columns={'Before': 'Avg Income Before (INR)', 'After': 'Avg Income After (INR)'}, inplace=True)\n",
    "    \n",
    "    if styled:\n",
    "        return summary_pivot.style.format({\n",
    "            'Avg Income Before (INR)': '₹{:,.2f}',\n",
    "            'Avg Income After (INR)': '₹{:,.2f}',\n",
    "            'Net_Income_Increase': '₹{:,.2f}',\n",
    "            'Pct_Income_Increase': '{:.2f}%'\n",
    "        })\n",
    "    else:\n",
    "        # Return the raw dataframe for plotting\n",
    "        return summary_pivot\n",
    "\n",
    "# --- HOW TO USE ---\n",
    "# Load your data\n",
    "file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# --- 1. Final Evaluation by District ---\n",
    "print(\"--- Final Income Evaluation by District ---\")\n",
    "# Get the styled table for display\n",
    "district_summary_styled = generate_impact_summary(df, 'District', styled=True)\n",
    "display(district_summary_styled)\n",
    "\n",
    "# Get the raw data for plotting the graph\n",
    "district_summary_raw = generate_impact_summary(df, 'District', styled=False)\n",
    "\n",
    "# **NEW: Plot the comparison graph for Districts**\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=district_summary_raw.index, y=district_summary_raw['Net_Income_Increase'], palette='plasma')\n",
    "plt.title('Net Annual Income Increase per District', fontsize=16)\n",
    "plt.ylabel('Average Income Increase (INR)')\n",
    "plt.xlabel('District')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. Final Evaluation by Primary Crop ---\n",
    "print(\"\\n--- Final Income Evaluation by Primary Crop ---\")\n",
    "# Get the styled table for display\n",
    "crop_summary_styled = generate_impact_summary(df, 'Primary_Crop', styled=True)\n",
    "display(crop_summary_styled)\n",
    "\n",
    "# Get the raw data for plotting the graph\n",
    "crop_summary_raw = generate_impact_summary(df, 'Primary_Crop', styled=False)\n",
    "\n",
    "# **NEW: Plot the comparison graph for Crops**\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=crop_summary_raw.index, y=crop_summary_raw['Net_Income_Increase'], palette='magma')\n",
    "plt.title('Net Annual Income Increase per Primary Crop', fontsize=16)\n",
    "plt.ylabel('Average Income Increase (INR)')\n",
    "plt.xlabel('Primary Crop')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ea0fe-db85-493f-87f1-9d573dfb8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data enriching special library \n",
    "\n",
    "## Layer 1: The Geospatial Data Foundation\n",
    "Your current CSV is your starting point. We need to enrich it with real-world data from satellites.\n",
    "\n",
    "1. How to Get Satellite Data:\n",
    "You'll use a platform like the Google Earth Engine (GEE), which has a powerful Python API. GEE provides free access to decades of satellite imagery from sources like Landsat and Sentinel-2. You can also explore data from ISRO's Bhuvan portal.\n",
    "\n",
    "2. What Features to Extract:\n",
    "For each farmer's location (which you can approximate from the district/village), you will write a Python script to calculate these indices for the \"Before\" and \"After\" periods:\n",
    "\n",
    "NDVI (Normalized Difference Vegetation Index): The most crucial index. It's a direct measure of plant health and density. You will be able to show that after the intervention, the NDVI of the farmland increased.\n",
    "\n",
    "NDWI (Normalized Difference Water Index): This measures water content and can show the impact of irrigation projects (like check dams built under WDC 2.0).\n",
    "\n",
    "Soil Moisture: Some datasets provide information on soil moisture, which is directly related to agricultural productivity.\n",
    "\n",
    "Your Augmented Dataset will look like this:\n",
    "\n",
    "Farmer_ID\tIncome_Annual\tLand_Size\t...\tAvg_NDVI_Before\tAvg_NDVI_After\tNDWI_Change\n",
    "FARM0001\t133212.18\t2.29\t...\t0.45\t0.68\t0.15\n",
    "\n",
    "Export to Sheets\n",
    "## Layer 2: The Advanced ML Engine\n",
    "With this incredibly rich dataset, a simple model won't do.\n",
    "\n",
    "The Predictive Model: You will use a competition-winning algorithm like XGBoost or LightGBM. You will train it on the full augmented dataset (including the satellite features) to predict Income_Annual. This model will be far more accurate because it's learning from real-world ground conditions.\n",
    "\n",
    "The Causal Model: You will still use the statistical model we built earlier (the linear regression) to calculate the net financial impact (e.g., the ₹14,570 increase).\n",
    "\n",
    "This two-model approach is a winning strategy: one model for raw predictive accuracy, and another for statistical proof of impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c36054-ae3f-42cd-9414-f44bf95fa121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install earthengine-api pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e9560-455f-40e1-b1b0-3769c7a56b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ee\n",
    "\n",
    "# --- Authenticate and Initialize GEE ---\n",
    "# This will trigger a prompt in your browser the first time you run it.\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Google Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Please run 'earthengine authenticate' in your terminal or command prompt first.\")\n",
    "    # In your terminal, run: earthengine authenticate\n",
    "    # Then re-run this cell.\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# --- Load Your Geocoded Data ---\n",
    "# This is the file you created in the previous step\n",
    "geocoded_file = 'fpo_data_geocoded.csv'\n",
    "df = pd.read_csv(geocoded_file)\n",
    "print(f\"Loaded {len(df)} records from {geocoded_file}\")\n",
    "\n",
    "\n",
    "# --- Define a Function to Get NDVI ---\n",
    "def get_ndvi_for_location(lat, lon, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculates the average NDVI for a specific location and time period.\n",
    "    \"\"\"\n",
    "    # Define the location point\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    \n",
    "    # Load the Sentinel-2 satellite image collection\n",
    "    image_collection = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "                        .filterBounds(point)\n",
    "                        .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "                        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))) # Filter for clear images\n",
    "    \n",
    "    # Function to calculate NDVI on a single image\n",
    "    def calculate_ndvi(image):\n",
    "        return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    \n",
    "    # Calculate NDVI for all images in the collection\n",
    "    ndvi_collection = image_collection.map(calculate_ndvi)\n",
    "    \n",
    "    # Get the average NDVI value over the time period\n",
    "    mean_ndvi = ndvi_collection.mean()\n",
    "    \n",
    "    # Extract the NDVI value at the specific point. The 500 is the scale in meters.\n",
    "    ndvi_value = mean_ndvi.reduceRegion(ee.Reducer.mean(), point, 500).get('NDVI')\n",
    "    \n",
    "    # The result from GEE is on their servers; .getInfo() brings it to your computer\n",
    "    return ndvi_value.getInfo()\n",
    "\n",
    "\n",
    "# --- Example Usage for the First Farmer ---\n",
    "# Let's get the data for the first farmer in your dataset\n",
    "first_farmer = df.iloc[0]\n",
    "latitude = first_farmer['Latitude']\n",
    "longitude = first_farmer['Longitude']\n",
    "\n",
    "print(f\"\\nFetching satellite data for the first farmer at ({latitude:.4f}, {longitude:.4f})...\")\n",
    "\n",
    "# Define your \"Before\" and \"After\" time periods\n",
    "# For this example, let's assume 'Before' is 2018 and 'After' is 2022\n",
    "before_start = '2018-01-01'\n",
    "before_end = '2018-12-31'\n",
    "after_start = '2022-01-01'\n",
    "after_end = '2022-12-31'\n",
    "\n",
    "# Get the NDVI values\n",
    "ndvi_before = get_ndvi_for_location(latitude, longitude, before_start, before_end)\n",
    "ndvi_after = get_ndvi_for_location(latitude, longitude, after_start, after_end)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Average NDVI 'Before' (2018): {ndvi_before:.4f}\")\n",
    "print(f\"Average NDVI 'After' (2022): {ndvi_after:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4c7a3-fc07-4249-9d15-973836530ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# --- Permanent Authentication using a Service Account Key ---\n",
    "\n",
    "# 1. Define the path to your JSON key file.\n",
    "# Make sure the file is in the same folder as your notebook!\n",
    "KEY_FILE_PATH = 'my-secret-key.json' # Or use the original long name\n",
    "\n",
    "# 2. Create credentials using the key file.\n",
    "credentials = ee.ServiceAccountCredentials(\n",
    "    'your-service-account-email@your-project-id.iam.gserviceaccount.com', # Replace with your service account's email\n",
    "    key_file=KEY_FILE_PATH\n",
    ")\n",
    "\n",
    "# 3. Initialize Earth Engine with these credentials and your project.\n",
    "ee.Initialize(credentials, project='ee-devanshibansal74')\n",
    "\n",
    "# --- Confirmation Message ---\n",
    "print(\"✅ Google Earth Engine initialized successfully using Service Account!\")\n",
    "\n",
    "# --- The rest of your script can now run without any interruptions ---\n",
    "try:\n",
    "    geocoded_file = 'fpo_data_geocoded.csv'\n",
    "    df = pd.read_csv(geocoded_file)\n",
    "    print(f\"Loaded {len(df)} records from {geocoded_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Make sure the file '{geocoded_file}' is in the same folder as your notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e0273-794f-4b75-8f8c-84a8b31bec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "print(\"--- Creating the required geocoded file ---\")\n",
    "\n",
    "# Load your original Excel dataset\n",
    "try:\n",
    "    file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"Successfully loaded the Excel file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"🛑 ERROR: The original Excel file was not found on your Desktop.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the geocoder\n",
    "geolocator = Nominatim(user_agent=\"sih_agri_project_v5\")\n",
    "\n",
    "# Get unique districts\n",
    "unique_districts = df['District'].unique()\n",
    "coordinate_map = {}\n",
    "\n",
    "print(f\"Finding coordinates for {len(unique_districts)} unique districts...\")\n",
    "for district in unique_districts:\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{district}, India\")\n",
    "        if location:\n",
    "            coordinate_map[district] = (location.latitude, location.longitude)\n",
    "        else:\n",
    "            coordinate_map[district] = (np.nan, np.nan)\n",
    "        time.sleep(1) # Pause to respect API limits\n",
    "    except Exception:\n",
    "        coordinate_map[district] = (np.nan, np.nan)\n",
    "\n",
    "# Add new Latitude and Longitude columns\n",
    "df['Latitude'] = df['District'].map(lambda d: coordinate_map.get(d, (np.nan, np.nan))[0])\n",
    "df['Longitude'] = df['District'].map(lambda d: coordinate_map.get(d, (np.nan, np.nan))[1])\n",
    "\n",
    "# Save the new file\n",
    "output_filename = 'fpo_data_geocoded.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ SUCCESS: The file '{output_filename}' has been created and saved in your project folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86801219-5b2c-4ca2-9516-563518b718c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Permanent Authentication ---\n",
    "# Make sure to replace these with your actual file name and email\n",
    "KEY_FILE_PATH = 'my-secret-key.json'\n",
    "SERVICE_ACCOUNT_EMAIL = 'your-service-account-email@your-project-id.iam.gserviceaccount.com'\n",
    "CLOUD_PROJECT = 'ee-devanshibansal74' # Your Google Cloud Project ID\n",
    "\n",
    "try:\n",
    "    credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT_EMAIL, key_file=KEY_FILE_PATH)\n",
    "    ee.Initialize(credentials, project=CLOUD_PROJECT)\n",
    "    print(\"✅ Google Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"🛑 Error initializing Earth Engine: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load Your Geocoded Data ---\n",
    "try:\n",
    "    geocoded_file = 'fpo_data_geocoded.csv'\n",
    "    df = pd.read_csv(geocoded_file)\n",
    "    print(f\"Loaded {len(df)} records from {geocoded_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"🛑 Error: Make sure the file '{geocoded_file}' is in the same folder as your notebook.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Get Unique Locations from Your Dataset ---\n",
    "unique_locations = df[['District', 'Latitude', 'Longitude']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Found {len(unique_locations)} unique districts to process.\")\n",
    "\n",
    "# --- Step 4: Define Image Processing Function ---\n",
    "def process_period(region, start_date, end_date):\n",
    "    \"\"\"Creates a cloud-free median NDVI image for a given region and time period.\"\"\"\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "                  .filterBounds(region)\n",
    "                  .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)))\n",
    "    \n",
    "    composite_image = collection.median()\n",
    "    ndvi_image = composite_image.normalizedDifference(['B8', 'B4'])\n",
    "    return ndvi_image\n",
    "\n",
    "# --- Step 5: Loop Through All Districts and Generate Visuals ---\n",
    "print(\"\\n--- Generating Satellite Visuals for All Districts ---\")\n",
    "print(\"This will take several minutes to complete...\")\n",
    "\n",
    "ndvi_viz_params = {'min': 0.0, 'max': 0.8, 'palette': ['#8B4513', '#FFFF00', '#00FF00', '#006400']}\n",
    "results = []\n",
    "\n",
    "for index, row in unique_locations.iterrows():\n",
    "    district_name = row['District']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    \n",
    "    print(f\"\\nProcessing: {district_name}...\")\n",
    "    \n",
    "    region = ee.Geometry.Point(lon, lat).buffer(10000)\n",
    "    \n",
    "    before_start, before_end = '2018-01-01', '2018-12-31'\n",
    "    after_start, after_end = '2022-01-01', '2022-12-31'\n",
    "    \n",
    "    ndvi_before = process_period(region, before_start, before_end)\n",
    "    ndvi_after = process_period(region, after_start, after_end)\n",
    "    \n",
    "    url_before = ndvi_before.getThumbURL({'params': ndvi_viz_params, 'dimensions': 800, 'region': region.bounds().getInfo()['coordinates']})\n",
    "    url_after = ndvi_after.getThumbURL({'params': ndvi_viz_params, 'dimensions': 800, 'region': region.bounds().getInfo()['coordinates']})\n",
    "    \n",
    "    results.append({'District': district_name, 'URL_Before': url_before, 'URL_After': url_after})\n",
    "\n",
    "# --- Step 6: Print All Generated URLs ---\n",
    "print(\"\\n\\n--- COMPLETE: Impactful Satellite Image URLs for All Districts ---\")\n",
    "for item in results:\n",
    "    print(\"\\n--------------------------------------------------\")\n",
    "    print(f\"District: {item['District']}\")\n",
    "    print(f\"🔗 NDVI Map 'Before' (2018): {item['URL_Before']}\")\n",
    "    print(f\"🔗 NDVI Map 'After' (2022): {item['URL_After']}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7585f368-bb32-465a-bd00-627066ee4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Error initializing Earth Engine: [Errno 2] No such file or directory: 'my-secret-key.json'\n",
      "Please ensure 'my-secret-key.json' is in your folder and the email/project ID are correct.\n",
      "🛑 Error: The file 'fpo_data_geocoded.csv' was not found. Please run the geocoding script first.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     exit()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# --- Step 3: Get Unique Locations ---\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m unique_locations \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistrict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_locations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique districts to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# --- Step 4: Define Image Processing Function ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Permanent Authentication ---\n",
    "# Make sure your JSON key file and service account email are correct.\n",
    "KEY_FILE_PATH = 'my-secret-key.json' \n",
    "SERVICE_ACCOUNT_EMAIL = 'gee-sih-project-runner@ee-devanshibansal74.iam.gserviceaccount.com'\n",
    "CLOUD_PROJECT = 'ee-devanshibansal74' \n",
    "\n",
    "try:\n",
    "    credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT_EMAIL, key_file=KEY_FILE_PATH)\n",
    "    ee.Initialize(credentials, project=CLOUD_PROJECT)\n",
    "    print(\"✅ Google Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"🛑 Error initializing Earth Engine: {e}\")\n",
    "    print(\"Please ensure 'my-secret-key.json' is in your folder and the email/project ID are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load Your Geocoded Data ---\n",
    "try:\n",
    "    geocoded_file = 'fpo_data_geocoded.csv'\n",
    "    df = pd.read_csv(geocoded_file)\n",
    "    print(f\"Loaded {len(df)} records from {geocoded_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"🛑 Error: The file '{geocoded_file}' was not found. Please run the geocoding script first.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Get Unique Locations ---\n",
    "unique_locations = df[['District', 'Latitude', 'Longitude']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Found {len(unique_locations)} unique districts to process.\")\n",
    "\n",
    "# --- Step 4: Define Image Processing Function ---\n",
    "def process_period(region, start_date, end_date):\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "                  .filterBounds(region)\n",
    "                  .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)))\n",
    "    \n",
    "    composite_image = collection.median()\n",
    "    ndvi_image = composite_image.normalizedDifference(['B8', 'B4'])\n",
    "    return ndvi_image\n",
    "\n",
    "# --- Step 5: Loop Through All Districts ---\n",
    "print(\"\\n--- Generating Satellite Visuals for All Districts ---\")\n",
    "\n",
    "# Define visualization parameters for the NDVI maps\n",
    "ndvi_viz_params = {\n",
    "    'min': 0.0,\n",
    "    'max': 0.8,\n",
    "    'palette': ['#8B4513', '#FFFF00', '#00FF00', '#006400']\n",
    "}\n",
    "results = []\n",
    "\n",
    "for index, row in unique_locations.iterrows():\n",
    "    district_name = row['District']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    \n",
    "    print(f\"\\nProcessing: {district_name}...\")\n",
    "    \n",
    "    region = ee.Geometry.Point(lon, lat).buffer(10000)\n",
    "    region_bounds = region.bounds().getInfo()['coordinates']\n",
    "    \n",
    "    before_start, before_end = '2018-01-01', '2018-12-31'\n",
    "    after_start, after_end = '2022-01-01', '2022-12-31'\n",
    "    \n",
    "    ndvi_before = process_period(region, before_start, before_end)\n",
    "    ndvi_after = process_period(region, after_start, after_end)\n",
    "    \n",
    "    # **THIS IS THE CORRECTED PART**\n",
    "    # We combine the visualization parameters with the other parameters into one dictionary.\n",
    "    params_before = ndvi_viz_params.copy()\n",
    "    params_before.update({'dimensions': 800, 'region': region_bounds})\n",
    "    \n",
    "    params_after = ndvi_viz_params.copy()\n",
    "    params_after.update({'dimensions': 800, 'region': region_bounds})\n",
    "    \n",
    "    url_before = ndvi_before.getThumbURL(params_before)\n",
    "    url_after = ndvi_after.getThumbURL(params_after)\n",
    "    \n",
    "    results.append({'District': district_name, 'URL_Before': url_before, 'URL_After': url_after})\n",
    "\n",
    "# --- Step 6: Print All Generated URLs ---\n",
    "print(\"\\n\\n--- COMPLETE: Impactful Satellite Image URLs for All Districts ---\")\n",
    "for item in results:\n",
    "    print(\"\\n--------------------------------------------------\")\n",
    "    print(f\"District: {item['District']}\")\n",
    "    print(f\"🔗 NDVI Map 'Before' (2018): {item['URL_Before']}\")\n",
    "    print(f\"🔗 NDVI Map 'After' (2022): {item['URL_After']}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a736722-ece4-4d1a-8fb9-96c77cbb3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Geocoding Farm Locations ---\n",
      "✅ Successfully loaded the original Excel file.\n",
      "Found 7 unique districts to process...\n",
      "✅ Geocoding complete. The file 'fpo_data_geocoded.csv' has been created.\n",
      "\n",
      "--- Part 2: Fetching Satellite Imagery ---\n",
      "🛑 FATAL ERROR: The key file 'my-secret-key.json' was not found.\n",
      "Please download your JSON key, move it to this folder, and rename it.\n",
      "🛑 FATAL ERROR: Could not initialize Earth Engine: [Errno 2] No such file or directory: 'my-secret-key.json'\n",
      "✅ Setup complete. Ready to fetch images for 7 districts.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import ee\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: CREATE THE GEOCODED DATA FILE\n",
    "# ==============================================================================\n",
    "print(\"--- Part 1: Geocoding Farm Locations ---\")\n",
    "\n",
    "# Load your original Excel dataset\n",
    "try:\n",
    "    file_path = \"C:/Users/Devanshi Bansal/Desktop/fpo_impact_data_enhanced.csv.xlsx\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"✅ Successfully loaded the original Excel file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"🛑 FATAL ERROR: The original Excel file was not found on your Desktop. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# Geocode the unique districts\n",
    "geolocator = Nominatim(user_agent=\"sih_agri_project_v6\")\n",
    "unique_districts = df['District'].unique()\n",
    "coordinate_map = {}\n",
    "\n",
    "print(f\"Found {len(unique_districts)} unique districts to process...\")\n",
    "for district in unique_districts:\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{district}, India\")\n",
    "        if location:\n",
    "            coordinate_map[district] = (location.latitude, location.longitude)\n",
    "        else:\n",
    "            coordinate_map[district] = (np.nan, np.nan)\n",
    "        time.sleep(1) \n",
    "    except Exception:\n",
    "        coordinate_map[district] = (np.nan, np.nan)\n",
    "\n",
    "# Add Latitude and Longitude columns\n",
    "df['Latitude'] = df['District'].map(lambda d: coordinate_map.get(d, (np.nan, np.nan))[0])\n",
    "df['Longitude'] = df['District'].map(lambda d: coordinate_map.get(d, (np.nan, np.nan))[1])\n",
    "\n",
    "# Save the geocoded data\n",
    "geocoded_file = 'fpo_data_geocoded.csv'\n",
    "df.to_csv(geocoded_file, index=False)\n",
    "print(f\"✅ Geocoding complete. The file '{geocoded_file}' has been created.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: FETCH SATELLITE IMAGERY\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 2: Fetching Satellite Imagery ---\")\n",
    "\n",
    "# --- Verify JSON Key File ---\n",
    "KEY_FILE_PATH = 'my-secret-key.json'\n",
    "if not os.path.exists(KEY_FILE_PATH):\n",
    "    print(f\"🛑 FATAL ERROR: The key file '{KEY_FILE_PATH}' was not found.\")\n",
    "    print(\"Please download your JSON key, move it to this folder, and rename it.\")\n",
    "    exit()\n",
    "\n",
    "# --- Authenticate and Initialize GEE ---\n",
    "try:\n",
    "    SERVICE_ACCOUNT_EMAIL = 'gee-sih-project-runner@ee-devanshibansal74.iam.gserviceaccount.com'\n",
    "    CLOUD_PROJECT = 'ee-devanshibansal74'\n",
    "    credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT_EMAIL, key_file=KEY_FILE_PATH)\n",
    "    ee.Initialize(credentials, project=CLOUD_PROJECT)\n",
    "    print(\"✅ Google Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"🛑 FATAL ERROR: Could not initialize Earth Engine: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Load Geocoded Data and Fetch Images ---\n",
    "df_geocoded = pd.read_csv(geocoded_file)\n",
    "unique_locations = df_geocoded[['District', 'Latitude', 'Longitude']].dropna().drop_duplicates()\n",
    "results = []\n",
    "# ... (The rest of the satellite fetching code will go here after this part runs successfully) ...\n",
    "print(f\"✅ Setup complete. Ready to fetch images for {len(unique_locations)} districts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96302ae8-fe5f-4f65-9c2b-3a9831df1881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
